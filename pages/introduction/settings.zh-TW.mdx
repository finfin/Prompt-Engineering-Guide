# LLM 設定

在使用提示時，您可以通過 API 或直接與 LLM 進行互動。您可以配置一些參數以獲得不同的提示結果。

**temperature** - 簡而言之，`temperature`越低，結果在某種程度上就越具有確定性，即總是選擇最有可能的下一個 token。提高 temperature 可能導致更多的隨機性，從而鼓勵產生更多元化或創意性的輸出。您實質上是在增加其他可能 token 的權重。實務上，基於事實的問答任務，您可能希望使用較低的 temperature 以鼓勵更多的事實和簡潔回應。對於詩歌生成或其他創意任務，提高 temperature 可能會有效果。

**top_p** - 同樣，使用名為核心採樣的帶溫度採樣技術 `top_p`，您可以控制模型在生成回應時的確定性程度。如果您正在尋找確切且基於事實的答案，請保持這個值較低。如果您正在尋找更多元化的回應，請將其提高到較高的值。

一般建議是更改其中一個，而不是兩者。

在開始一些基本範例之前，請注意您使用的 LLM 版本可能會影響您的結果。